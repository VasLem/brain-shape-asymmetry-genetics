\chapter{Materials and Methods}\label{chap:mat_and_methods}
In figure \ref{} a brief overview of the processes applied in this work is displayed. In the coming sections, each compartment will be separately analyzed.

\begin{figure}
	content...
\end{figure}


\section{Data description}
\subsection{Primary Data Source}
With the advent of technology capable to collect and process genomes from different individuals in relatively high speed, vast databases targeting human physiology have been constructed. One of the main players in the data collection has been UK Biobank; a large-scale database from a randomized consortium of 500,000 individuals, whose genome has been collected, from whom  48,000 subjects had also participated in brain \ac{mri} collection process, as of December 2020 \cite{Littlejohns2020}. Participants were male and female, with the age range spanning 40 to 69. The study focuses on healthy self-reported white individuals of European ancestry, filtering and preprocessing them based on the work of \citet{Naqvi2021}. Specifically, the discovery dataset, amounting to 19,644 individuals, of the present work was identical to the one used in their study. In addition to that data, a smaller one, coming from a different, collected at a later time, batch, of 16,342 individuals was used as a replication dataset during \ac{gwas}, reapplying with the same filtering and preprocessing steps. More information about the signal-specific preprocessing is provided in the next sections.


\section{Methods applied on Phenotype}
\subsection{Initial filtering and preprocessing}
\label{subsec:pheno_preproc}
T1-weighted MRI scans are analyzed. The analysis is performed by initially converting the raw DCIM MRI volumetric images to well-defined 3D surface triangular meshes through the pipeline applied by FreeSurfer `recon-all` \cite{Reuter2012} and ciftify `ciftify-recon-all` commands \cite{Dickie2019}, on a space of 32 thousand vertices, with the average edge length being 2mm. Subsequently, the mid-cortical surface is arbitrarily selected, purportedly because of its plausible smoothness, allowing the distinction of sulci and gyri without over-representing their geometry \cite{Naqvi2021}. After quality control,  the vertices from the sub-cortical part of the surface, referring to corpus callosum connection points, were removed based on a mask derived from the Conte69 atlas \cite{Glasser2011}, getting reduced to 29,759.

\subsection{\Ac{mri} Shapes Normalization}\label{subsec:shape_normalization}
The current work applies principles from general symmetry studies to model cortical asymmetry. For any of these analyses to occur, the abberations of \ac{3d} shapes produced from \ac{mri} scans need to be considered. \Ac{mri} output is affected by the subject positioning and technical error \cite{Wittens2021}.  Volumetric differences also increase the level of discrepancies among \ac{mri} samples.  To prevent positioning and volume deviations from gravely affecting shape comparisons, a normalization is required\cite{Klingenberg2020}. The samples of the derived 3D triangular mesh are represented as a set of vertices $\mathcal{V_S}$ of predefined dimensionality P, with a single landmark coded in the format of (x,y,z) coordinates. Those are joined together with a predefined faces matrix $\mathcal{E_S}$, with each of its elements containing three indices referring to $\mathcal{V_S}$, with the additional constraint that $S$ is a multiple-connected structure, namely a graph in which there is at least one path joining any two vertices. Shapes normalization is performed through the application of \ac{gpa}. \Ac{gpa} is an algorithm that iteratively performs translation, scaling and rotation on a given set of structures $S$, given initially a reference $S_0$, aiming to minimize the euclidean distance of corresponding points and the average shape. The translation is performed in such a way that the centroid C, defined by $\frac{\sum_{\forall i}{\mathcal{G_S}}_i}{P}$, becomes the system origin. The scaling is such that the centroid size of the normalized $S$ structure, defined by $\sqrt{\sum_{\forall i}{\left|\left|{\mathcal{G_S}}_i-C\right|\right|_2}}$ becomes equal to 1. The transformed samples then belong to what it has been coined as Kendall Space \cite{Klingenberg2020}.  Under the framework of cortical surface analysis, a single hemisphere is considered to be one of the $S$ structures. To apply any symmetry analysis, therefore, one of the individual hemispheres needs to be mirrored on the other side of the midsagittal plane, and then \ac{gpa} is applied to align all hemispheres at once. The mirroring is performed by normalization of right and left hemispheres sets separately, and, then, x coordinate sign inversion of the right hemisphere landmarks. Aligning, finally, the entire dataset marks the end of the shapes normalization for the two tasks, statistical asymmetry analysis and \ac{gwas}.

\subsection{Shape downsampling}
\label{subsec:downsampling}
An intermediate step is followed when performing corticla symmetry statistical analysis, in order to reduce the computational burden of the process. A rather simple algorithm, in MATLAB context, has been derived, that computes the subset of indices of a given shape S, \textit{approximately} with a given factor, that best describe the downsampled shape $M$ provided from the  proprietary function `reducepatch` output \cite{Lopez2014}. From this method, the analyzed shapes were downsampled by a factor of 10, with the average shape retaining most morphological characteristics. The key idea is to find the one-to-many correspondence between faces from the two meshes. Let $\mathbf{{Cn}_T}$ be the centroids of each face of a shape T. The faces correspondence is found by identifying for each face $x\in G_M$ with index $i_x$ a part $S_x$ of S with $\mathcal{G_{S_x}}\subset G_S$ joined through a path length of 10, given the desired reduction rate to the closest face $y_{min}(x)=argmin_{\mathbf{{Cn}_Y}}\left|\left|\mathbf{{Cn}_Y}-\mathbf{{Cn}_M}\right|\right|$. Those elements are having non-zero entries in the 10th power of the S's adjacency matrix, at the $y_min$-th row. Then, the optimal vertex correspondence is found by taking all the vertices corresponding to the faces subset $\mathbf{V_{S_x}}$ and identifying which of them is the closest to each of the vertices of x. The resulting downsampled shape R then has the faces of M, but projected on the vertices of S. This mapping allows for instant, although naive and of reduced quality, downsampling of 29,759 to 3,098 landmarks (\autoref{fig:downsampling}).
\begin{figure}[H]
	\centering
	\includesvg[width=\linewidth]{downsampling/reduction}
	\caption{Downsampling indices of original average template using the novel algorithm and specific reduction scales (1/r). As no inter-face edge connectivity criterion is being considered, artifacts occur in the approximated shape, in the form of scars.}
	\label{fig:downsampling}
\end{figure}

\subsection{Symmetry Statistical analysis}
Bilateral asymmetry is mainly described using three components in literature \cite{klingenberg2002}\cite{Vingerhoets2021}. \Acf{da}, the main focus of this study, corresponds to the hemispheric side effect, namely how the intrinsic (i.e. genetic) properties of the studied population are manifesting across individuals. Antisymmetry, which is related to the effect where sidedness is random in a population (i.e. left-right pattern is mirrored to a right-left pattern), is not observed in the human cerebral cortex, in contrast to other internal organs positions, or organisms \cite{Neubauer2020}. The third component, \acf{fa}, encompasses any random developmental and environmental effects, that cannot be explained with the existing knowledge. The observed deviations can be statistically linearly modeled as products of two effects, the hemisphere side studied and the individual specimen analyzed, as well as their interaction \cite{klingenberg2002}. Formally, based on \cite{VanDongen1999} assuming the presence of replications for each observation per individual, to account for technical error, a mixed linear model representing the aforementioned dependencies is defined as:
\begin{equation}
Y_{ijk} = \mu + \beta + I_i + S_{ij} + E_{ijk}
\end{equation}
where $Y_{ijk}$ is the phenotype of the i-th individual, from the j-th side, under the k-th replication, $\mu$ and $\beta$ are the fixed intercept and fixed side effect respectively, $I_i\sim\mathcal{N}(0,\sigma^2_{ind})$ is the random individual effect,  $S_{ij}\sim\mathcal{N}(0,\sigma^2_{FA})$ is the random side and individual specific effect, matched to \ac{fa}, and $E_{ijk}\sim\mathcal{N}(0,\sigma^2_{ME})$ is the measurement error. Given this definition, a way to measure the statistical significance is performed through an F-test applied on a 2-way nonparametric permutation-based \ac{anova}, to relate the \ac{rss} ratios of effects to observable error terms, and of fluctuating effect to the measurement error. 

While simple \ac{anova} bases the significance of the result on the assumption of normality, permutation-based \ac{anova} makes no assumptions on the underlying distribution \cite{Anderson2001}. Instead it bases significance of a F-score on the number of permutations which resulted in F-scores higher than the F-score measured in the simple \ac{anova} scenario on the original data, divided by the total number of permutations \cite{Klingenberg1998}. Let N be the number of individuals. A reshaping operation is performed, after which the set of size L landmarks becomes a set of size 3L coordinates per individual per side, in other words a \ac{3d} dataset. The permutations were generated considering each time the dimension being investigated, pursuing biologically feasible result when possible; for assessing the individual effect, the hemispheres were randomly shuffled across individuals; for the side effect test, landmarks of each individual were reassigned a random side; for the fluctuating effect, the whole dataset was randomly permuted. As it can be foreseen, a handicap of the latter permutation is that is being sampled from a possible set of $(6NL)!$ configurations, whereas the two former ones from $(3NL)!$ and $(6L)!$ configurations respectively. This renders the last test more sensitive to assign low p-values to each landmark, as the possible configuration that could possibly produce a better f-score is less likely to be selected. However, it is worth noting that in all tests the possible cases number is prohibitively large, and that analyses in Monte Carlo simulations suggest the size of 1000 replications as good enough \cite{Marozzi2004}. The consecutive analysis has also been demonstrated in the work of \citet{Vanbiervliet2022}. 1000 replications were, therefore, selected to test the significance of each asymmetry component, which means that the analysis was computationally intensive, but facilitated by the downsampling described in \autoref{subsec:downsampling}. Five random subsets of 50 samples were collected from the discovery dataset. The number of samples was chosen experimentally, as it was observed that the size of 1000 replications is actually not enough for larger datasets and the method was generally sensitive in assigning high significance (p-value<0.05) to each landmark, the larger the set size assessed. A number of different random subsets was selected, so that to reduce the effect of cherry-picking. The final counts were computed to be the average of the experimental iterations. Although the last step is not theoretically correct, as independent pools of combinations are assessed, it achieves a smoothing operation CITATION NEEDED.

Replications are necessary in such analysis, in order to distinguish the \ac{fa} effect from measurement error. To this end, an \ac{mri} test-retest subset of 20 individuals from HCP was retrieved \cite{VanEssen2013}, and the preprocessing mentioned in \autoref{subsec:pheno_preproc} was performed. For each landmark and coordinate, and for each hemisphere separately, the mean observed replication variance across individuals was computed. Subsequently, assuming gaussian distribution with 0 mean for technical measurement error, an augmented dataset could be produced for the \ac{mri} samples in the UK Biobank dataset. Three replications per individual were generated by sampling from the identified distributions.


After measuring the F-score

1000 permutations were generated from the supplied dataset


Extra care needs to be given on the determination of the \ac{dof} of each term, given the preprocessing applied to bring the hemispheres surfaces into Kendall shape space \cite{klingenberg2002}. Given that the analysis is performed on a pair of symmetric objects, and not on a single symmetric object, this configuration is named \textbf{matching asymmetry analysis}.


The consecutive analysis has also been demonstrated in the work of . 

\subsection{Statistical asymmetry shapes augmentation}
In order to per

\subsection{\ac{gwas} covariates control}
In the case of \ac{gwas}, the goal is to remove any noise that could

\section{Shapes Partitioning}
The present work evaluates the brain asymmetry genetic landscape in a coarse-to-fine segmentation. The technique has been used in a number of different related studies \cite{Claes2018}\cite{Naqvi2021}, yielding results that are in accordance with the underlying anatomic features. The main reason behind this partitioning is the intrinsic complexity of the studied phenotype, eliciting expected differences in the genomic profiles of each cerebral cortex region. This type of distance-based clustering is governed by the least quantity of assumptions, regarding the shape or form of the cluster \cite{VonLuxburg2007}. The partitions' genetic juxtaposition is valuable for identifying which regions share similar significant genetic loci, highlighting the corresponding genes contribution, or showcasing the specialization of certain regions that share little to no similarities with their neighbors.

\Acf{hsc} is an unsupervised method of iterative partitioning, that makes use of the distance matrix eigenvectors \cite{Ng2002}. The distance matrix is constructed between pairs of values. It results into a binary tree structure (i.e. each parent shape is partitioned into two children).  In the current study, a level-4 partitioning is performed, resulting into 31 partitions. Subsequently, they are transformed to the corresponding principal components that explain 80\% of the variance, not only for reasons of further dimensionality reduction, but also to ensure that the resulting traits are orthogonal, and therefore compatible for \ac{ldsc} analyses


